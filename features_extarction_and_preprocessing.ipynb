{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 5\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start by loading the train/test dataframes. The `train_test_split` here is we found after hours of brainstorming some interesting examples using this split and the current `SEED'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=SEED)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
    "train_y.hist()\n",
    "valid_y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple picture to explain Diabetic Retinopathy\n",
    "\n",
    "How do we know that a patient have diabetic retinopahy? **[There are at least 5 things to spot on](https://www.eyeops.com/contents/our-services/eye-diseases/diabetic-retinopathy)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From quick investigations of the data we anlayzed and finalized three features **hemorrhages** , **cotton wool** spots, **hard exudates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Original Inputs\n",
    "\n",
    "First, let have a glance of original inputs. Each row depicts each severity level. We can see two problems which make the severity difficult to spot on. First, some images are very dark [pic(0,2) and pic(4,4) ] and sometimes different color illumination is confusing [pic (3,3)]. Second, we can get the uninformative dark areas for some pictures [pic(0,1), pic(0,3)]. This is important when we reduce the picture size, as informative areas become too small. So it is intuitive to crop the uninformative areas out in the second case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "# display 10 images from each class\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx, row) in enumerate(df_train.loc[df_train['diagnosis'] == class_id].sample(5, random_state=SEED).iterrows()):\n",
    "        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        plt.imshow(image)\n",
    "        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['id_code']) )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try gray scale and feel understand better for some pictures, as color distraction is gone. For example, we can see more blood clearer in the upper part of pic(4,4), which has severity of level 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx, row) in enumerate(df_train.loc[df_train['diagnosis'] == class_id].sample(5, random_state=SEED).iterrows()):\n",
    "        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#         image=cv2.addWeighted ( image, 0 , cv2.GaussianBlur( image , (0 ,0 ) , 10) ,-4 ,128)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['id_code']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dpi = 80 #inch\n",
    "\n",
    "# path=f\"../input/aptos2019-blindness-detection/train_images/5c7ab966a3ee.png\" # notice upper part\n",
    "path=f\"../input/aptos2019-blindness-detection/train_images/cd54d022e37d.png\" # lower-right, this still looks not so severe, can be class3\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "height, width = image.shape\n",
    "print(height, width)\n",
    "\n",
    "SCALE=2\n",
    "figsize = (width / float(dpi))/SCALE, (height / float(dpi))/SCALE\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepocessing Method\n",
    "\n",
    "We get our insughts from Ben's preprocessing pipline which doesn't actually do the full thing so we used crop_from_gray function to extarct even more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx, row) in enumerate(df_train.loc[df_train['diagnosis'] == class_id].sample(5, random_state=SEED).iterrows()):\n",
    "        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128) # the trick is to add this line\n",
    "\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['id_code']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Further improve by auto-cropping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OLD version of image color cropping, use crop_image_from_gray instead\n",
    "# The above code work only for 1-channel. Here is my simple extension for 3-channels image\n",
    "def crop_image(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        h,w,_=img.shape\n",
    "#         print(h,w)\n",
    "        img1=cv2.resize(crop_image1(img[:,:,0]),(w,h))\n",
    "        img2=cv2.resize(crop_image1(img[:,:,1]),(w,h))\n",
    "        img3=cv2.resize(crop_image1(img[:,:,2]),(w,h))\n",
    "        \n",
    "#         print(img1.shape,img2.shape,img3.shape)\n",
    "        img[:,:,0]=img1\n",
    "        img[:,:,1]=img2\n",
    "        img[:,:,2]=img3\n",
    "        return img\n",
    "\n",
    "'''all of these do not work'''\n",
    "\n",
    "def crop_image2(image,threshold=5):\n",
    "    if len(image.shape) == 3:\n",
    "        flatImage = np.max(image, 2)\n",
    "    else:\n",
    "        flatImage = image\n",
    "    assert len(flatImage.shape) == 2\n",
    "\n",
    "    rows = np.where(np.max(flatImage, 0) > threshold)[0]\n",
    "    if rows.size:\n",
    "        cols = np.where(np.max(flatImage, 1) > threshold)[0]\n",
    "        image = image[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
    "    else:\n",
    "        image = image[:1, :1]\n",
    "\n",
    "    return image\n",
    "\n",
    "def crop_image3(image):\n",
    "    mask = image > 0\n",
    "\n",
    "    # Coordinates of non-black pixels.\n",
    "    coords = np.argwhere(mask)\n",
    "\n",
    "    # Bounding box of non-black pixels.\n",
    "    x0, y0 = coords.min(axis=0)\n",
    "    x1, y1 = coords.max(axis=0) + 1   # slices are exclusive at the top\n",
    "    \n",
    "    # Get the contents of the bounding box.\n",
    "    cropped = image[x0:x1, y0:y1]\n",
    "    return cropped\n",
    "\n",
    "def crop_image4(image):\n",
    "    _,thresh = cv2.threshold(image,1,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    crop = image[y:y+h,x:x+w]\n",
    "    return crop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Cropping the images\n",
    "\n",
    "We have tested on around 200 images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Color Version of Cropping \n",
    "\n",
    "GAUSSIAN_BLUR used to normalize features and and sigmaX=10. Could increase it to 30 fro better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_ben_color(path, sigmaX=10):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_SAMP=7\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx, row) in enumerate(df_train.loc[df_train['diagnosis'] == class_id].sample(NUM_SAMP, random_state=SEED).iterrows()):\n",
    "        ax = fig.add_subplot(5, NUM_SAMP, class_id * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "        image = load_ben_color(path,sigmaX=30)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        ax.set_title('%d-%d-%s' % (class_id, idx, row['id_code']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.A2 Try the new idea of circle crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def circle_crop(img, sigmaX=10):   \n",
    "    \"\"\"\n",
    "    Create circular crop around image centre    \n",
    "    \"\"\"    \n",
    "    \n",
    "    img = cv2.imread(img)\n",
    "    img = crop_image_from_gray(img)    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    height, width, depth = img.shape    \n",
    "    \n",
    "    x = int(width/2)\n",
    "    y = int(height/2)\n",
    "    r = np.amin((x,y))\n",
    "    \n",
    "    circle_img = np.zeros((height, width), np.uint8)\n",
    "    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
    "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "    img = crop_image_from_gray(img)\n",
    "    img=cv2.addWeighted ( img,4, cv2.GaussianBlur( img , (0,0) , sigmaX) ,-4 ,128)\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## try circle crop\n",
    "NUM_SAMP=7\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx, row) in enumerate(df_train.loc[df_train['diagnosis'] == class_id].sample(NUM_SAMP, random_state=SEED).iterrows()):\n",
    "        ax = fig.add_subplot(5, NUM_SAMP, class_id * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n",
    "        image = circle_crop(path,sigmaX=30)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        ax.set_title('%d-%d-%s' % (class_id, idx, row['id_code']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can try plotting a picture (sample train pic(4,1) above) with IMG_SIZE with cropping, now important information is much clearer to see with `sigmaX = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dpi = 80 #inch\n",
    "\n",
    "# path=f\"../input/aptos2019-blindness-detection/train_images/5c7ab966a3ee.png\" # notice upper part\n",
    "path=f\"../input/aptos2019-blindness-detection/train_images/cd54d022e37d.png\" # lower-right, can be class3\n",
    "image = load_ben_color(path,sigmaX=10)\n",
    "\n",
    "height, width = IMG_SIZE, IMG_SIZE\n",
    "print(height, width)\n",
    "\n",
    "SCALE=1\n",
    "figsize = (width / float(dpi))/SCALE, (height / float(dpi))/SCALE\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vlaidation of our pipline using different public datasets.\n",
    "We can also try auto cropping on 50 test data to see that it work fine. Below, we see immediately from this random samples that severed cases, with level >2, are relatively many more compared to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_SAMP=10\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for jj in range(5):\n",
    "    for i, (idx, row) in enumerate(df_test.sample(NUM_SAMP,random_state=SEED+jj).iterrows()):\n",
    "        ax = fig.add_subplot(5, NUM_SAMP, jj * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/test_images/{row['id_code']}.png\"\n",
    "        image = load_ben_color(path,sigmaX=30)\n",
    "        \n",
    "        plt.imshow(image)\n",
    "        ax.set_title('%d-%s' % (idx, row['id_code']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''Bonus : sigmaX=50'''\n",
    "NUM_SAMP=10\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for jj in range(5):\n",
    "    for i, (idx, row) in enumerate(df_test.sample(NUM_SAMP,random_state=SEED+jj).iterrows()):\n",
    "        ax = fig.add_subplot(5, NUM_SAMP, jj * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/aptos2019-blindness-detection/test_images/{row['id_code']}.png\"\n",
    "        image = load_ben_color(path,sigmaX=50)\n",
    "\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        ax.set_title('%d-%s' % (idx, row['id_code']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the unpreprocess version, just for comparison and how we got from bad features extraction to a satisfactory version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMP=10\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for class_id in sorted(train_y.unique()):\n",
    "    for i, (idx, row) in enumerate(df_old.loc[df_old['level'] == class_id].sample(NUM_SAMP, random_state=SEED).iterrows()):\n",
    "        ax = fig.add_subplot(5, NUM_SAMP, class_id * NUM_SAMP + i + 1, xticks=[], yticks=[])\n",
    "        path=f\"../input/diabetic-retinopathy-resized/resized_train/resized_train/{row['image']}.jpeg\"\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         image = crop_image_from_gray(image)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "#         image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128)\n",
    "\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        ax.set_title('%d-%d-%s' % (class_id, idx, row['image']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok preprocessing methods seem to works fine; however, the doctors to estimate the severity levels in the past competitions may have different criteria in mind than the doctors of Aravind, so it is possible to have some estimation inconsistency (at least to my eyes the previous data seems more noisy). The following level-4 [pic(4,1) in the plot we just made above] looks not so severe. (Or this is the example case of too many blood vessels ??, refer to Section 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dpi = 80 #inch\n",
    "\n",
    "path=f\"../input/diabetic-retinopathy-resized/resized_train/resized_train/31590_right.jpeg\" # too many vessels?\n",
    "# path=f\"../input/diabetic-retinopathy-resized/resized_train/resized_train/18017_left.jpeg\" # details are lost\n",
    "image = load_ben_color(path,sigmaX=30)\n",
    "# image = cv2.imread(path)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# image = crop_image1(image)\n",
    "# image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "# image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128)\n",
    "\n",
    "height, width = IMG_SIZE, IMG_SIZE\n",
    "print(height, width)\n",
    "\n",
    "SCALE=1\n",
    "figsize = (width / float(dpi))/SCALE, (height / float(dpi))/SCALE\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A. Heatmap visualization\n",
    "![heatmap](https://i.ibb.co/6FM6VCC/gradcam-resized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B. Inconsistency of Ophthalmologist's Estimation\n",
    "![inconsistent  estimation in diabetic retinophaty](https://i.ibb.co/6rQ2sFG/inconsistent-estimation.png)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 875431,
     "sourceId": 14774,
     "sourceType": "competition"
    },
    {
     "datasetId": 131128,
     "sourceId": 418031,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 250877,
     "sourceId": 527603,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29282,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
